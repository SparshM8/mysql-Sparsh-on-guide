{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899aea9c",
   "metadata": {},
   "source": [
    "# Lab 14: Performance Tuning and Optimization - Interactive Practice\n",
    "\n",
    "This notebook provides hands-on practice with MySQL performance tuning and optimization techniques. You'll learn to analyze query performance, optimize indexes, tune configuration settings, and monitor database performance.\n",
    "\n",
    "## Learning Objectives\n",
    "- Analyze query performance using EXPLAIN plans\n",
    "- Design and implement effective indexing strategies\n",
    "- Optimize slow queries and identify bottlenecks\n",
    "- Configure MySQL server parameters for optimal performance\n",
    "- Monitor database performance and resource usage\n",
    "- Implement performance testing and benchmarking\n",
    "- Apply best practices for database optimization\n",
    "\n",
    "## Prerequisites\n",
    "- Lab 13: Database Triggers and Events\n",
    "- MySQL Server running with administrative access\n",
    "- Large dataset for performance testing\n",
    "- Python packages: mysql-connector-python, pandas, matplotlib\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install mysql-connector-python pandas matplotlib numpy\n",
    "\n",
    "# Import required libraries\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connection configuration\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'your_password',  # Replace with your MySQL password\n",
    "    'database': 'performance_test',\n",
    "    'autocommit': True\n",
    "}\n",
    "\n",
    "def create_connection():\n",
    "    \"\"\"Create database connection\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        print(\"‚úÖ Connected to MySQL database\")\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"‚ùå Error connecting to MySQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_query(connection, query, description=\"\", fetch=True, timing=False):\n",
    "    \"\"\"Execute a query and optionally return results as DataFrame\"\"\"\n",
    "    start_time = time.time() if timing else None\n",
    "    \n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        if description:\n",
    "            print(f\"\\n{description}\")\n",
    "        \n",
    "        if timing:\n",
    "            execution_time = time.time() - start_time\n",
    "            print(f\"‚è±Ô∏è  Execution time: {execution_time:.4f} seconds\")\n",
    "        \n",
    "        if fetch and cursor.description:\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            \n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows, columns=columns)\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No results returned\")\n",
    "                return None\n",
    "        else:\n",
    "            cursor.close()\n",
    "            print(\"Query executed successfully\")\n",
    "            return None\n",
    "            \n",
    "    except Error as e:\n",
    "        print(f\"‚ùå Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_performance_test(connection, test_name, iterations=10):\n",
    "    \"\"\"Run a performance test with timing\"\"\"\n",
    "    print(f\"\\nüß™ Running performance test: {test_name}\")\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"SELECT COUNT(*) FROM customers WHERE state = 'CA'\",\n",
    "        \"SELECT COUNT(*) FROM customers c JOIN orders o ON c.id = o.customer_id WHERE c.state = 'CA' AND o.total_amount > 100\",\n",
    "        \"SELECT AVG(total_amount) FROM orders WHERE customer_id IN (SELECT id FROM customers WHERE state = 'CA' LIMIT 100)\"\n",
    "    ]\n",
    "    \n",
    "    total_time = 0\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for query in queries:\n",
    "            start_time = time.time()\n",
    "            execute_query(connection, query, fetch=False)\n",
    "            total_time += time.time() - start_time\n",
    "    \n",
    "    avg_time = total_time / (iterations * len(queries))\n",
    "    print(f\"üìä Average query time: {avg_time:.4f} seconds\")\n",
    "    print(f\"üöÄ Queries per second: {1/avg_time:.2f}\")\n",
    "    \n",
    "    return avg_time\n",
    "\n",
    "# Test connection\n",
    "conn = create_connection()\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Connection test successful\")\n",
    "else:\n",
    "    print(\"‚ùå Please check your database configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ad7c",
   "metadata": {},
   "source": [
    "## Exercise 1: Setting Up Performance Test Database\n",
    "\n",
    "Let's create the performance test database and generate sample data for our optimization exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12220e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Enable performance monitoring\n",
    "    execute_query(conn, \"SET GLOBAL performance_schema = ON\", \n",
    "                  description=\"Enabling performance schema\", fetch=False)\n",
    "    execute_query(conn, \"SET GLOBAL slow_query_log = 'ON'\", \n",
    "                  description=\"Enabling slow query log\", fetch=False)\n",
    "    execute_query(conn, \"SET GLOBAL long_query_time = 1\", \n",
    "                  description=\"Setting slow query threshold to 1 second\", fetch=False)\n",
    "    \n",
    "    # Create database\n",
    "    execute_query(conn, \"CREATE DATABASE IF NOT EXISTS performance_test\", \n",
    "                  description=\"Creating performance_test database\", fetch=False)\n",
    "    execute_query(conn, \"USE performance_test\", \n",
    "                  description=\"Switching to performance_test database\", fetch=False)\n",
    "    \n",
    "    # Create tables\n",
    "    tables_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customers (\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "        name VARCHAR(100),\n",
    "        email VARCHAR(100),\n",
    "        city VARCHAR(50),\n",
    "        state VARCHAR(2),\n",
    "        zip_code VARCHAR(10),\n",
    "        created_date DATE,\n",
    "        last_login TIMESTAMP,\n",
    "        INDEX idx_name (name),\n",
    "        INDEX idx_email (email),\n",
    "        INDEX idx_city_state (city, state)\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "        customer_id INT,\n",
    "        order_date DATE,\n",
    "        total_amount DECIMAL(10,2),\n",
    "        status ENUM('pending', 'processing', 'shipped', 'delivered'),\n",
    "        shipping_address TEXT,\n",
    "        FOREIGN KEY (customer_id) REFERENCES customers(id),\n",
    "        INDEX idx_customer_date (customer_id, order_date),\n",
    "        INDEX idx_status_date (status, order_date),\n",
    "        INDEX idx_total (total_amount)\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS order_items (\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "        order_id INT,\n",
    "        product_id INT,\n",
    "        quantity INT,\n",
    "        unit_price DECIMAL(8,2),\n",
    "        discount DECIMAL(5,2) DEFAULT 0,\n",
    "        FOREIGN KEY (order_id) REFERENCES orders(id),\n",
    "        INDEX idx_order_product (order_id, product_id),\n",
    "        INDEX idx_product (product_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    execute_query(conn, tables_sql, \n",
    "                  description=\"Creating test tables\", fetch=False)\n",
    "    \n",
    "    # Generate sample data (smaller dataset for faster testing)\n",
    "    print(\"\\nüìä Generating sample data...\")\n",
    "    \n",
    "    # Insert customers\n",
    "    customers_data = []\n",
    "    for i in range(1, 1001):  # 1000 customers\n",
    "        customers_data.append((\n",
    "            f'Customer {i}',\n",
    "            f'customer{i}@example.com',\n",
    "            ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'][i % 5],\n",
    "            ['NY', 'CA', 'IL', 'TX', 'AZ'][i % 5],\n",
    "            f'{i:05d}',\n",
    "            (datetime.now() - timedelta(days=np.random.randint(1, 365))).date(),\n",
    "            datetime.now() - timedelta(days=np.random.randint(1, 30))\n",
    "        ))\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(\n",
    "        \"INSERT INTO customers (name, email, city, state, zip_code, created_date, last_login) VALUES (%s, %s, %s, %s, %s, %s, %s)\",\n",
    "        customers_data\n",
    "    )\n",
    "    print(f\"‚úÖ Inserted {len(customers_data)} customers\")\n",
    "    \n",
    "    # Insert orders\n",
    "    orders_data = []\n",
    "    for i in range(1, 5001):  # 5000 orders\n",
    "        orders_data.append((\n",
    "            np.random.randint(1, 1001),  # customer_id\n",
    "            (datetime.now() - timedelta(days=np.random.randint(1, 365))).date(),\n",
    "            round(np.random.uniform(10, 1000), 2),\n",
    "            ['pending', 'processing', 'shipped', 'delivered'][np.random.randint(0, 4)],\n",
    "            f'Address for order {i}'\n",
    "        ))\n",
    "    \n",
    "    cursor.executemany(\n",
    "        \"INSERT INTO orders (customer_id, order_date, total_amount, status, shipping_address) VALUES (%s, %s, %s, %s, %s)\",\n",
    "        orders_data\n",
    "    )\n",
    "    print(f\"‚úÖ Inserted {len(orders_data)} orders\")\n",
    "    \n",
    "    # Insert order items\n",
    "    items_data = []\n",
    "    for i in range(1, 20001):  # 20,000 order items\n",
    "        items_data.append((\n",
    "            np.random.randint(1, 5001),  # order_id\n",
    "            np.random.randint(1, 1001),  # product_id\n",
    "            np.random.randint(1, 11),    # quantity\n",
    "            round(np.random.uniform(5, 200), 2),  # unit_price\n",
    "            round(np.random.uniform(0, 20), 2)     # discount\n",
    "        ))\n",
    "    \n",
    "    cursor.executemany(\n",
    "        \"INSERT INTO order_items (order_id, product_id, quantity, unit_price, discount) VALUES (%s, %s, %s, %s, %s)\",\n",
    "        items_data\n",
    "    )\n",
    "    print(f\"‚úÖ Inserted {len(items_data)} order items\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    # Check table sizes\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT table_name, table_rows \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'performance_test' AND table_name IN ('customers', 'orders', 'order_items')\n",
    "    \"\"\", description=\"Checking table sizes\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf3860",
   "metadata": {},
   "source": [
    "## Exercise 2: Query Analysis with EXPLAIN\n",
    "\n",
    "Let's analyze query performance using EXPLAIN plans to understand how MySQL executes queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489977d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Basic EXPLAIN examples\n",
    "    explain_queries = [\n",
    "        (\"Simple primary key lookup\", \"EXPLAIN SELECT * FROM customers WHERE id = 1\"),\n",
    "        (\"Index scan on name\", \"EXPLAIN SELECT * FROM customers WHERE name LIKE 'Customer 1%'\"),\n",
    "        (\"Composite index usage\", \"EXPLAIN SELECT * FROM customers WHERE city = 'New York' AND state = 'NY'\"),\n",
    "        (\"JOIN without optimization\", \"\"\"EXPLAIN SELECT c.name, COUNT(o.id) as order_count\n",
    "                                         FROM customers c\n",
    "                                         LEFT JOIN orders o ON c.id = o.customer_id\n",
    "                                         WHERE c.state = 'CA'\n",
    "                                         GROUP BY c.id, c.name\n",
    "                                         ORDER BY order_count DESC LIMIT 10\"\"\")\n",
    "    ]\n",
    "    \n",
    "    for description, query in explain_queries:\n",
    "        df = execute_query(conn, query, description=f\"{description}:\")\n",
    "        if df is not None:\n",
    "            display(df)\n",
    "    \n",
    "    # Analyze query execution time\n",
    "    print(\"\\n‚è±Ô∏è  Query Performance Comparison:\")\n",
    "    \n",
    "    test_queries = [\n",
    "        (\"Simple count\", \"SELECT COUNT(*) FROM customers\"),\n",
    "        (\"Filtered count\", \"SELECT COUNT(*) FROM customers WHERE state = 'CA'\"),\n",
    "        (\"JOIN query\", \"SELECT COUNT(*) FROM customers c JOIN orders o ON c.id = o.customer_id WHERE c.state = 'CA'\")\n",
    "    ]\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for description, query in test_queries:\n",
    "        start_time = time.time()\n",
    "        execute_query(conn, query, fetch=False)\n",
    "        execution_time = time.time() - start_time\n",
    "        performance_results.append((description, execution_time))\n",
    "        print(f\"{description}: {execution_time:.4f} seconds\")\n",
    "    \n",
    "    # Visualize performance\n",
    "    if performance_results:\n",
    "        df_perf = pd.DataFrame(performance_results, columns=['Query', 'Time'])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(df_perf['Query'], df_perf['Time'])\n",
    "        plt.title('Query Performance Comparison')\n",
    "        plt.ylabel('Execution Time (seconds)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97b0e8",
   "metadata": {},
   "source": [
    "## Exercise 3: Index Optimization\n",
    "\n",
    "Let's create and analyze the impact of different indexing strategies on query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Check existing indexes\n",
    "    df = execute_query(conn, \"SHOW INDEXES FROM customers\",\n",
    "                      description=\"Existing indexes on customers table\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Performance test before adding indexes\n",
    "    print(\"\\nüìä Baseline Performance Test:\")\n",
    "    baseline_time = run_performance_test(conn, \"Baseline (no additional indexes)\", iterations=5)\n",
    "    \n",
    "    # Add strategic indexes\n",
    "    index_queries = [\n",
    "        \"CREATE INDEX idx_customers_state ON customers (state)\",\n",
    "        \"CREATE INDEX idx_orders_customer_amount ON orders (customer_id, total_amount)\",\n",
    "        \"CREATE INDEX idx_orders_date_status ON orders (order_date, status)\",\n",
    "        \"CREATE INDEX idx_order_items_order_quantity ON order_items (order_id, quantity)\"\n",
    "    ]\n",
    "    \n",
    "    for query in index_queries:\n",
    "        execute_query(conn, query, description=\"Creating index\", fetch=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Indexes created successfully\")\n",
    "    \n",
    "    # Performance test after adding indexes\n",
    "    print(\"\\nüìä Performance Test After Index Optimization:\")\n",
    "    optimized_time = run_performance_test(conn, \"After Index Optimization\", iterations=5)\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if baseline_time > 0:\n",
    "        improvement = ((baseline_time - optimized_time) / baseline_time) * 100\n",
    "        print(f\"\\nüéØ Performance Improvement: {improvement:.1f}%\")\n",
    "        print(f\"‚ö° Speed increase: {baseline_time/optimized_time:.1f}x faster\")\n",
    "    \n",
    "    # Analyze index usage\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        object_name,\n",
    "        index_name,\n",
    "        count_read,\n",
    "        count_fetch,\n",
    "        count_insert,\n",
    "        count_update,\n",
    "        count_delete\n",
    "    FROM performance_schema.table_io_waits_summary_by_index_usage\n",
    "    WHERE object_schema = 'performance_test'\n",
    "    AND count_read > 0\n",
    "    ORDER BY count_read DESC\n",
    "    \"\"\", description=\"Index usage statistics\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Test specific query optimizations\n",
    "    print(\"\\nüîç Specific Query Optimizations:\")\n",
    "    \n",
    "    # Query 1: Customer search by state\n",
    "    explain_before = execute_query(conn, \"EXPLAIN SELECT COUNT(*) FROM customers WHERE state = 'CA'\",\n",
    "                                  description=\"EXPLAIN: State-based customer count\")\n",
    "    \n",
    "    # Query 2: High-value orders\n",
    "    explain_join = execute_query(conn, \"\"\"EXPLAIN SELECT c.name, o.total_amount\n",
    "                                         FROM customers c \n",
    "                                         JOIN orders o ON c.id = o.customer_id \n",
    "                                         WHERE c.state = 'CA' AND o.total_amount > 500\"\"\")\n",
    "    \n",
    "    # Query 3: Recent orders analysis\n",
    "    explain_agg = execute_query(conn, \"\"\"EXPLAIN SELECT DATE_FORMAT(order_date, '%Y-%m') as month, \n",
    "                                         COUNT(*) as orders, SUM(total_amount) as revenue\n",
    "                                         FROM orders \n",
    "                                         WHERE order_date >= '2023-01-01'\n",
    "                                         GROUP BY DATE_FORMAT(order_date, '%Y-%m')\"\"\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083cd27",
   "metadata": {},
   "source": [
    "## Exercise 4: Query Optimization Techniques\n",
    "\n",
    "Let's explore different query optimization techniques and their impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37128f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Subquery vs JOIN optimization\n",
    "    print(\"üîÑ Subquery vs JOIN Optimization:\")\n",
    "    \n",
    "    # Inefficient subquery\n",
    "    subquery_sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM customers c\n",
    "    WHERE c.id IN (\n",
    "        SELECT customer_id\n",
    "        FROM orders\n",
    "        WHERE total_amount > 500\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimized JOIN\n",
    "    join_sql = \"\"\"\n",
    "    SELECT DISTINCT c.*\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.id = o.customer_id\n",
    "    WHERE o.total_amount > 500\n",
    "    \"\"\"\n",
    "    \n",
    "    # EXISTS optimization\n",
    "    exists_sql = \"\"\"\n",
    "    SELECT c.*\n",
    "    FROM customers c\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 FROM orders o\n",
    "        WHERE o.customer_id = c.id\n",
    "        AND o.total_amount > 500\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    queries_to_test = [\n",
    "        (\"Subquery\", subquery_sql),\n",
    "        (\"JOIN\", join_sql),\n",
    "        (\"EXISTS\", exists_sql)\n",
    "    ]\n",
    "    \n",
    "    optimization_results = []\n",
    "    \n",
    "    for name, query in queries_to_test:\n",
    "        start_time = time.time()\n",
    "        df = execute_query(conn, query)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        result_count = len(df) if df is not None else 0\n",
    "        optimization_results.append((name, execution_time, result_count))\n",
    "        print(f\"{name}: {execution_time:.4f}s, {result_count} results\")\n",
    "    \n",
    "    # Visualize optimization results\n",
    "    if optimization_results:\n",
    "        df_opt = pd.DataFrame(optimization_results, columns=['Method', 'Time', 'Results'])\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        ax1.bar(df_opt['Method'], df_opt['Time'])\n",
    "        ax1.set_title('Execution Time by Method')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        \n",
    "        ax2.bar(df_opt['Method'], df_opt['Results'])\n",
    "        ax2.set_title('Result Count by Method')\n",
    "        ax2.set_ylabel('Number of Results')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # LIMIT optimization\n",
    "    print(\"\\nüìè LIMIT Optimization:\")\n",
    "    \n",
    "    limit_queries = [\n",
    "        (\"Without LIMIT\", \"SELECT * FROM orders ORDER BY total_amount DESC\"),\n",
    "        (\"With LIMIT 10\", \"SELECT * FROM orders ORDER BY total_amount DESC LIMIT 10\"),\n",
    "        (\"With LIMIT 100\", \"SELECT * FROM orders ORDER BY total_amount DESC LIMIT 100\")\n",
    "    ]\n",
    "    \n",
    "    limit_results = []\n",
    "    \n",
    "    for name, query in limit_queries:\n",
    "        start_time = time.time()\n",
    "        df = execute_query(conn, query)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        result_count = len(df) if df is not None else 0\n",
    "        limit_results.append((name, execution_time, result_count))\n",
    "        print(f\"{name}: {execution_time:.4f}s, {result_count} results\")\n",
    "    \n",
    "    # UNION vs UNION ALL\n",
    "    print(\"\\nüîó UNION vs UNION ALL:\")\n",
    "    \n",
    "    union_sql = \"\"\"\n",
    "    SELECT customer_id, 'high_value' as category FROM orders WHERE total_amount > 500\n",
    "    UNION\n",
    "    SELECT customer_id, 'frequent' as category FROM orders GROUP BY customer_id HAVING COUNT(*) > 10\n",
    "    \"\"\"\n",
    "    \n",
    "    union_all_sql = \"\"\"\n",
    "    SELECT customer_id, 'high_value' as category FROM orders WHERE total_amount > 500\n",
    "    UNION ALL\n",
    "    SELECT customer_id, 'frequent' as category FROM orders GROUP BY customer_id HAVING COUNT(*) > 10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test UNION\n",
    "    start_time = time.time()\n",
    "    df_union = execute_query(conn, union_sql)\n",
    "    union_time = time.time() - start_time\n",
    "    \n",
    "    # Test UNION ALL\n",
    "    start_time = time.time()\n",
    "    df_union_all = execute_query(conn, union_all_sql)\n",
    "    union_all_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"UNION: {union_time:.4f}s, {len(df_union) if df_union is not None else 0} results\")\n",
    "    print(f\"UNION ALL: {union_all_time:.4f}s, {len(df_union_all) if df_union_all is not None else 0} results\")\n",
    "    \n",
    "    if union_all_time > 0:\n",
    "        speedup = union_time / union_all_time\n",
    "        print(f\"UNION ALL is {speedup:.1f}x faster\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d834cd1",
   "metadata": {},
   "source": [
    "## Exercise 5: MySQL Configuration Tuning\n",
    "\n",
    "Let's analyze and optimize MySQL server configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60499db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Check current configuration\n",
    "    config_vars = [\n",
    "        'innodb_buffer_pool_size',\n",
    "        'innodb_log_file_size',\n",
    "        'max_connections',\n",
    "        'query_cache_size',\n",
    "        'tmp_table_size',\n",
    "        'max_heap_table_size',\n",
    "        'slow_query_log',\n",
    "        'long_query_time'\n",
    "    ]\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Current MySQL Configuration:\")\n",
    "    for var in config_vars:\n",
    "        df = execute_query(conn, f\"SHOW VARIABLES LIKE '{var}'\")\n",
    "        if df is not None and not df.empty:\n",
    "            value = df.iloc[0]['Value']\n",
    "            # Convert bytes to MB for readability\n",
    "            if var in ['innodb_buffer_pool_size', 'innodb_log_file_size', 'query_cache_size', 'tmp_table_size', 'max_heap_table_size']:\n",
    "                try:\n",
    "                    mb_value = int(value) / 1024 / 1024\n",
    "                    print(f\"{var}: {mb_value:.1f} MB\")\n",
    "                except:\n",
    "                    print(f\"{var}: {value}\")\n",
    "            else:\n",
    "                print(f\"{var}: {value}\")\n",
    "    \n",
    "    # Get system information for recommendations\n",
    "    print(\"\\nüíª System Information:\")\n",
    "    \n",
    "    # Check available memory (simplified - in real scenarios you'd check system specs)\n",
    "    df = execute_query(conn, \"SELECT @@version as mysql_version\")\n",
    "    if df is not None:\n",
    "        print(f\"MySQL Version: {df.iloc[0]['mysql_version']}\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    print(\"\\nüìã Performance Tuning Recommendations:\")\n",
    "    print(\"1. Buffer Pool Size: Should be 70-80% of available RAM\")\n",
    "    print(\"2. Max Connections: Based on application requirements\")\n",
    "    print(\"3. Query Cache: Enable for read-heavy workloads (MySQL 5.7 and earlier)\")\n",
    "    print(\"4. Temporary Tables: Increase for complex queries\")\n",
    "    print(\"5. Slow Query Log: Enable for performance monitoring\")\n",
    "    \n",
    "    # Apply some optimizations (be careful in production!)\n",
    "    print(\"\\nüîß Applying Configuration Optimizations:\")\n",
    "    \n",
    "    # Note: These are examples - adjust based on your system\n",
    "    optimizations = [\n",
    "        (\"Setting max_connections\", \"SET GLOBAL max_connections = 200\"),\n",
    "        (\"Setting tmp_table_size\", \"SET GLOBAL tmp_table_size = 134217728\"),  # 128MB\n",
    "        (\"Setting max_heap_table_size\", \"SET GLOBAL max_heap_table_size = 134217728\"),  # 128MB\n",
    "        (\"Configuring slow query log\", \"SET GLOBAL slow_query_log = 'ON'\"),\n",
    "        (\"Setting slow query threshold\", \"SET GLOBAL long_query_time = 2\")\n",
    "    ]\n",
    "    \n",
    "    for description, query in optimizations:\n",
    "        try:\n",
    "            execute_query(conn, query, fetch=False)\n",
    "            print(f\"‚úÖ {description}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {description}: {str(e)}\")\n",
    "    \n",
    "    # Test configuration impact\n",
    "    print(\"\\nüß™ Testing Configuration Impact:\")\n",
    "    \n",
    "    # Run performance test with new configuration\n",
    "    config_time = run_performance_test(conn, \"After Configuration Tuning\", iterations=3)\n",
    "    \n",
    "    # Check current status\n",
    "    df = execute_query(conn, \"SHOW ENGINE INNODB STATUS\", fetch=False)\n",
    "    \n",
    "    # Check process list\n",
    "    df = execute_query(conn, \"SHOW PROCESSLIST\",\n",
    "                      description=\"Current database connections\")\n",
    "    if df is not None:\n",
    "        print(f\"Active connections: {len(df)}\")\n",
    "    \n",
    "    # Check open tables\n",
    "    df = execute_query(conn, \"SHOW OPEN TABLES\",\n",
    "                      description=\"Open tables status\")\n",
    "    if df is not None:\n",
    "        print(f\"Open tables: {len(df)}\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aea14c",
   "metadata": {},
   "source": [
    "## Exercise 6: Performance Monitoring\n",
    "\n",
    "Let's implement comprehensive performance monitoring and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Create performance monitoring dashboard\n",
    "    print(\"üìä Performance Monitoring Dashboard\")\n",
    "    \n",
    "    # System overview\n",
    "    df = execute_query(conn, \"SELECT @@version as mysql_version, @@innodb_buffer_pool_size/1024/1024 as buffer_pool_mb\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Connection status\n",
    "    df = execute_query(conn, \"SELECT COUNT(*) as active_connections, SUM(time) as total_connection_time FROM information_schema.processlist\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Database size information\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        table_schema,\n",
    "        ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) as size_mb,\n",
    "        COUNT(*) as tables_count\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema NOT IN ('information_schema', 'performance_schema', 'mysql')\n",
    "    GROUP BY table_schema\n",
    "    ORDER BY size_mb DESC\n",
    "    \"\"\", description=\"Database size information\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Table-specific statistics\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT\n",
    "        table_name,\n",
    "        table_rows,\n",
    "        ROUND(data_length/1024/1024, 2) as data_mb,\n",
    "        ROUND(index_length/1024/1024, 2) as index_mb,\n",
    "        ROUND((data_length + index_length)/1024/1024, 2) as total_mb\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'performance_test'\n",
    "    ORDER BY data_length DESC\n",
    "    \"\"\", description=\"Table statistics\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "        \n",
    "        # Visualize table sizes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(df['table_name'], df['total_mb'])\n",
    "        plt.title('Table Size Distribution')\n",
    "        plt.ylabel('Size (MB)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Index usage analysis\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT\n",
    "        object_name,\n",
    "        index_name,\n",
    "        count_read,\n",
    "        count_fetch,\n",
    "        ROUND(count_read / (count_read + count_fetch + 1) * 100, 2) as usage_pct\n",
    "    FROM performance_schema.table_io_waits_summary_by_index_usage\n",
    "    WHERE object_schema = 'performance_test'\n",
    "    AND count_read > 0\n",
    "    ORDER BY count_read DESC\n",
    "    LIMIT 10\n",
    "    \"\"\", description=\"Top 10 most used indexes\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Slow query analysis\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT\n",
    "        sql_text,\n",
    "        exec_count,\n",
    "        ROUND(avg_timer_wait/1000000000, 3) as avg_time_sec,\n",
    "        ROUND(sum_timer_wait/1000000000, 3) as total_time_sec\n",
    "    FROM performance_schema.events_statements_summary_by_digest\n",
    "    WHERE avg_timer_wait > 1000000000 -- > 1 second average\n",
    "    ORDER BY sum_timer_wait DESC\n",
    "    LIMIT 5\n",
    "    \"\"\", description=\"Slowest queries (last 5)\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Memory usage\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT\n",
    "        event_name,\n",
    "        ROUND(current_alloc/1024/1024, 2) as current_mb,\n",
    "        ROUND(high_alloc/1024/1024, 2) as peak_mb\n",
    "    FROM sys.memory_global_by_current_bytes\n",
    "    WHERE current_alloc > 1024*1024 -- > 1MB\n",
    "    ORDER BY current_alloc DESC\n",
    "    LIMIT 10\n",
    "    \"\"\", description=\"Top memory consumers\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Create performance summary report\n",
    "    print(\"\\nüìà Performance Summary Report\")\n",
    "    \n",
    "    # Overall health score (simplified)\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_indexes,\n",
    "        SUM(CASE WHEN count_read > 0 THEN 1 ELSE 0 END) as used_indexes\n",
    "    FROM performance_schema.table_io_waits_summary_by_index_usage\n",
    "    WHERE object_schema = 'performance_test'\n",
    "    \"\"\", fetch=False)\n",
    "    \n",
    "    # Calculate some metrics\n",
    "    df = execute_query(conn, \"SELECT COUNT(*) as slow_queries FROM performance_schema.events_statements_summary_by_digest WHERE avg_timer_wait > 1000000000\")\n",
    "    slow_queries = df.iloc[0]['slow_queries'] if df is not None and not df.empty else 0\n",
    "    \n",
    "    df = execute_query(conn, \"SELECT COUNT(*) as total_connections FROM information_schema.processlist\")\n",
    "    connections = df.iloc[0]['total_connections'] if df is not None and not df.empty else 0\n",
    "    \n",
    "    print(f\"Slow queries (>1s): {slow_queries}\")\n",
    "    print(f\"Active connections: {connections}\")\n",
    "    print(f\"Performance monitoring: {'Enabled' if slow_queries >= 0 else 'Disabled'}\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41db2fb",
   "metadata": {},
   "source": [
    "## Exercise 7: Advanced Optimization Techniques\n",
    "\n",
    "Let's implement advanced optimization techniques like partitioning and summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17678605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Table partitioning\n",
    "    print(\"üìä Implementing Table Partitioning:\")\n",
    "    \n",
    "    # Partition orders table by year\n",
    "    partition_sql = \"\"\"\n",
    "    ALTER TABLE orders\n",
    "    PARTITION BY RANGE (YEAR(order_date)) (\n",
    "        PARTITION p2020 VALUES LESS THAN (2021),\n",
    "        PARTITION p2021 VALUES LESS THAN (2022),\n",
    "        PARTITION p2022 VALUES LESS THAN (2023),\n",
    "        PARTITION p2023 VALUES LESS THAN (2024),\n",
    "        PARTITION p2024 VALUES LESS THAN (2025),\n",
    "        PARTITION p_future VALUES LESS THAN MAXVALUE\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        execute_query(conn, partition_sql, fetch=False)\n",
    "        print(\"‚úÖ Orders table partitioned successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Partitioning failed (may already be partitioned): {str(e)}\")\n",
    "    \n",
    "    # Check partition information\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT\n",
    "        table_name,\n",
    "        partition_name,\n",
    "        table_rows,\n",
    "        ROUND(data_length/1024/1024, 2) as data_mb\n",
    "    FROM information_schema.partitions\n",
    "    WHERE table_schema = 'performance_test'\n",
    "    AND table_name = 'orders'\n",
    "    ORDER BY partition_ordinal_position\n",
    "    \"\"\", description=\"Partition information\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Test partition pruning\n",
    "    print(\"\\nüéØ Testing Partition Pruning:\")\n",
    "    \n",
    "    partition_tests = [\n",
    "        (\"All data\", \"SELECT COUNT(*) FROM orders\"),\n",
    "        (\"2024 data only\", \"SELECT COUNT(*) FROM orders WHERE order_date >= '2024-01-01'\"),\n",
    "        (\"2023 data only\", \"SELECT COUNT(*) FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31'\")\n",
    "    ]\n",
    "    \n",
    "    for description, query in partition_tests:\n",
    "        df = execute_query(conn, f\"EXPLAIN {query}\", description=f\"EXPLAIN: {description}\")\n",
    "        if df is not None:\n",
    "            # Check if partitions are mentioned in the explain plan\n",
    "            partitions_used = \"partitions\" in str(df.to_string()).lower()\n",
    "            print(f\"Partition pruning: {'Yes' if partitions_used else 'No'}\")\n",
    "    \n",
    "    # Create summary tables\n",
    "    print(\"\\nüìã Creating Summary Tables:\")\n",
    "    \n",
    "    # Monthly sales summary\n",
    "    summary_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS monthly_sales_summary (\n",
    "        year_month VARCHAR(7) PRIMARY KEY,\n",
    "        total_orders INT DEFAULT 0,\n",
    "        total_revenue DECIMAL(12,2) DEFAULT 0,\n",
    "        avg_order_value DECIMAL(10,2) DEFAULT 0,\n",
    "        unique_customers INT DEFAULT 0,\n",
    "        last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    execute_query(conn, summary_sql, description=\"Creating monthly sales summary table\", fetch=False)\n",
    "    \n",
    "    # Populate summary table\n",
    "    populate_sql = \"\"\"\n",
    "    INSERT INTO monthly_sales_summary (year_month, total_orders, total_revenue, avg_order_value, unique_customers, last_updated)\n",
    "    SELECT\n",
    "        DATE_FORMAT(order_date, '%Y-%m') as year_month,\n",
    "        COUNT(DISTINCT o.id) as total_orders,\n",
    "        ROUND(SUM(o.total_amount), 2) as total_revenue,\n",
    "        ROUND(AVG(o.total_amount), 2) as avg_order_value,\n",
    "        COUNT(DISTINCT o.customer_id) as unique_customers,\n",
    "        NOW() as last_updated\n",
    "    FROM orders o\n",
    "    GROUP BY DATE_FORMAT(order_date, '%Y-%m')\n",
    "    ORDER BY year_month\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        total_orders = VALUES(total_orders),\n",
    "        total_revenue = VALUES(total_revenue),\n",
    "        avg_order_value = VALUES(avg_order_value),\n",
    "        unique_customers = VALUES(unique_customers),\n",
    "        last_updated = NOW()\n",
    "    \"\"\"\n",
    "    \n",
    "    execute_query(conn, populate_sql, description=\"Populating monthly sales summary\", fetch=False)\n",
    "    \n",
    "    # Compare query performance\n",
    "    print(\"\\n‚ö° Performance Comparison: Original vs Summary Table\")\n",
    "    \n",
    "    # Query using original table\n",
    "    original_query = \"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(order_date, '%Y-%m') as month,\n",
    "        COUNT(DISTINCT id) as orders,\n",
    "        ROUND(SUM(total_amount), 2) as revenue,\n",
    "        ROUND(AVG(total_amount), 2) as avg_order\n",
    "    FROM orders \n",
    "    WHERE order_date >= '2023-01-01'\n",
    "    GROUP BY DATE_FORMAT(order_date, '%Y-%m')\n",
    "    ORDER BY month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query using summary table\n",
    "    summary_query = \"\"\"\n",
    "    SELECT \n",
    "        year_month as month,\n",
    "        total_orders as orders,\n",
    "        total_revenue as revenue,\n",
    "        avg_order_value as avg_order\n",
    "    FROM monthly_sales_summary\n",
    "    WHERE year_month >= '2023-01'\n",
    "    ORDER BY year_month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Time original query\n",
    "    start_time = time.time()\n",
    "    df_original = execute_query(conn, original_query)\n",
    "    original_time = time.time() - start_time\n",
    "    \n",
    "    # Time summary query\n",
    "    start_time = time.time()\n",
    "    df_summary = execute_query(conn, summary_query)\n",
    "    summary_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Original table query: {original_time:.4f} seconds\")\n",
    "    print(f\"Summary table query: {summary_time:.4f} seconds\")\n",
    "    \n",
    "    if original_time > 0:\n",
    "        speedup = original_time / summary_time\n",
    "        print(f\"Summary table is {speedup:.1f}x faster!\")\n",
    "    \n",
    "    # Verify results are the same\n",
    "    if df_original is not None and df_summary is not None:\n",
    "        results_match = df_original.equals(df_summary.rename(columns={'year_month': 'month', 'total_orders': 'orders', 'total_revenue': 'revenue', 'avg_order_value': 'avg_order'}))\n",
    "        print(f\"Results match: {results_match}\")\n",
    "    \n",
    "    # Display summary data\n",
    "    df = execute_query(conn, \"SELECT * FROM monthly_sales_summary ORDER BY year_month DESC LIMIT 5\",\n",
    "                      description=\"Recent monthly sales summary\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4f50d",
   "metadata": {},
   "source": [
    "## Exercise 8: Performance Testing and Benchmarking\n",
    "\n",
    "Let's create a comprehensive performance testing framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b48dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = create_connection()\n",
    "\n",
    "if conn:\n",
    "    # Create performance testing framework\n",
    "    print(\"üß™ Creating Performance Testing Framework\")\n",
    "    \n",
    "    # Create results table\n",
    "    results_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS performance_test_results (\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "        test_name VARCHAR(100),\n",
    "        test_scenario VARCHAR(100),\n",
    "        iterations INT,\n",
    "        avg_query_time DECIMAL(10,6),\n",
    "        total_time DECIMAL(10,6),\n",
    "        queries_per_second DECIMAL(10,2),\n",
    "        test_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        notes TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    execute_query(conn, results_table_sql, description=\"Creating performance test results table\", fetch=False)\n",
    "    \n",
    "    # Create performance testing procedure\n",
    "    test_procedure_sql = \"\"\"\n",
    "    DELIMITER $$\n",
    "    \n",
    "    CREATE PROCEDURE run_performance_test(\n",
    "        IN test_name VARCHAR(100),\n",
    "        IN test_scenario VARCHAR(100),\n",
    "        IN iterations INT\n",
    "    )\n",
    "    BEGIN\n",
    "        DECLARE i INT DEFAULT 1;\n",
    "        DECLARE start_time TIMESTAMP;\n",
    "        DECLARE total_time DECIMAL(10,6);\n",
    "        DECLARE avg_time DECIMAL(10,6);\n",
    "        DECLARE qps DECIMAL(10,2);\n",
    "        \n",
    "        SET start_time = NOW();\n",
    "        SET total_time = 0;\n",
    "        \n",
    "        -- Run test iterations\n",
    "        WHILE i <= iterations DO\n",
    "            -- Test Query 1: Simple customer lookup\n",
    "            SELECT SQL_NO_CACHE COUNT(*) INTO @count1 \n",
    "            FROM customers \n",
    "            WHERE state = 'CA';\n",
    "            \n",
    "            -- Test Query 2: Customer orders JOIN\n",
    "            SELECT SQL_NO_CACHE COUNT(*) INTO @count2\n",
    "            FROM customers c\n",
    "            JOIN orders o ON c.id = o.customer_id\n",
    "            WHERE c.state = 'CA' AND o.total_amount > 100;\n",
    "            \n",
    "            -- Test Query 3: Aggregation query\n",
    "            SELECT SQL_NO_CACHE AVG(total_amount) INTO @avg_amount\n",
    "            FROM orders\n",
    "            WHERE customer_id IN (\n",
    "                SELECT id FROM customers WHERE state = 'CA' LIMIT 100\n",
    "            );\n",
    "            \n",
    "            SET i = i + 1;\n",
    "        END WHILE;\n",
    "        \n",
    "        -- Calculate metrics\n",
    "        SET total_time = TIMESTAMPDIFF(MICROSECOND, start_time, NOW()) / 1000000;\n",
    "        SET avg_time = total_time / (iterations * 3); -- 3 queries per iteration\n",
    "        SET qps = (iterations * 3) / total_time;\n",
    "        \n",
    "        -- Store results\n",
    "        INSERT INTO performance_test_results \n",
    "        (test_name, test_scenario, iterations, avg_query_time, total_time, queries_per_second)\n",
    "        VALUES (test_name, test_scenario, iterations, avg_time, total_time, qps);\n",
    "        \n",
    "        -- Return results\n",
    "        SELECT \n",
    "            test_name,\n",
    "            test_scenario,\n",
    "            iterations,\n",
    "            ROUND(avg_time, 6) as avg_query_time,\n",
    "            ROUND(total_time, 3) as total_time_seconds,\n",
    "            ROUND(qps, 2) as queries_per_second,\n",
    "            CONCAT('Test completed: ', iterations * 3, ' queries in ', ROUND(total_time, 3), ' seconds') as status;\n",
    "    END$$\n",
    "    \n",
    "    DELIMITER ;\n",
    "    \"\"\"\n",
    "    \n",
    "    execute_query(conn, test_procedure_sql, description=\"Creating performance testing procedure\", fetch=False)\n",
    "    \n",
    "    # Run baseline test\n",
    "    print(\"\\nüìä Running Baseline Performance Test\")\n",
    "    df = execute_query(conn, \"CALL run_performance_test('Baseline Test', 'No Optimizations', 50)\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Add more indexes for optimization test\n",
    "    additional_indexes = [\n",
    "        \"CREATE INDEX idx_orders_status_date ON orders (status, order_date)\",\n",
    "        \"CREATE INDEX idx_customers_created_state ON customers (created_date, state)\",\n",
    "        \"CREATE INDEX idx_order_items_product_quantity ON order_items (product_id, quantity)\"\n",
    "    ]\n",
    "    \n",
    "    for index_sql in additional_indexes:\n",
    "        execute_query(conn, index_sql, description=\"Adding optimization index\", fetch=False)\n",
    "    \n",
    "    # Run optimized test\n",
    "    print(\"\\nüìä Running Optimized Performance Test\")\n",
    "    df = execute_query(conn, \"CALL run_performance_test('Optimized Test', 'With Additional Indexes', 50)\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    \n",
    "    # Compare results\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        test_name,\n",
    "        test_scenario,\n",
    "        iterations,\n",
    "        ROUND(avg_query_time * 1000, 3) as avg_query_time_ms,\n",
    "        ROUND(queries_per_second, 2) as queries_per_second,\n",
    "        test_date\n",
    "    FROM performance_test_results\n",
    "    ORDER BY test_date DESC\n",
    "    LIMIT 2\n",
    "    \"\"\", description=\"Performance test comparison\")\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if len(df) >= 2:\n",
    "            baseline_qps = df.iloc[1]['queries_per_second']  # Older test\n",
    "            optimized_qps = df.iloc[0]['queries_per_second']  # Newer test\n",
    "            \n",
    "            if baseline_qps > 0:\n",
    "                improvement = ((optimized_qps - baseline_qps) / baseline_qps) * 100\n",
    "                print(f\"\\nüéØ Performance Improvement: {improvement:.1f}%\")\n",
    "                print(f\"‚ö° Queries per second: {baseline_qps:.1f} ‚Üí {optimized_qps:.1f}\")\n",
    "    \n",
    "    # Visualize performance trends\n",
    "    df = execute_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        test_name,\n",
    "        queries_per_second,\n",
    "        test_date\n",
    "    FROM performance_test_results\n",
    "    ORDER BY test_date\n",
    "    \"\"\", description=\"Performance trend data\")\n",
    "    if df is not None and len(df) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['test_name'], df['queries_per_second'], marker='o')\n",
    "        plt.title('Performance Test Results')\n",
    "        plt.ylabel('Queries per Second')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a6b63",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Query Analysis**: Using EXPLAIN plans to understand query execution\n",
    "2. **Index Optimization**: Creating and managing database indexes effectively\n",
    "3. **Query Optimization**: Various techniques to improve query performance\n",
    "4. **Configuration Tuning**: Optimizing MySQL server settings\n",
    "5. **Performance Monitoring**: Tracking and analyzing database performance\n",
    "6. **Advanced Techniques**: Partitioning, summary tables, and caching\n",
    "7. **Performance Testing**: Creating benchmarks and measuring improvements\n",
    "\n",
    "### Key Takeaways:\n",
    "- Always analyze queries with EXPLAIN before optimization\n",
    "- Proper indexing is crucial for query performance\n",
    "- Configuration should match your workload requirements\n",
    "- Regular monitoring helps identify performance issues early\n",
    "- Summary tables can dramatically improve reporting query performance\n",
    "- Partitioning helps manage large tables efficiently\n",
    "\n",
    "### Performance Optimization Hierarchy:\n",
    "1. **Fix the query** - Optimize SQL statements first\n",
    "2. **Add indexes** - Ensure proper indexing\n",
    "3. **Tune configuration** - Adjust server settings\n",
    "4. **Use summary tables** - Pre-aggregate common queries\n",
    "5. **Implement partitioning** - For very large tables\n",
    "6. **Consider hardware upgrades** - As a last resort\n",
    "\n",
    "### Next Steps:\n",
    "- **Lab 15**: Database Security and User Management\n",
    "- **Lab 16**: Backup, Recovery, and High Availability\n",
    "- Apply these techniques to your production databases\n",
    "- Set up automated monitoring and alerting\n",
    "- Consider MySQL Enterprise features for advanced optimization\n",
    "\n",
    "Remember: Performance optimization is an ongoing process. Monitor your database regularly and adjust as your workload changes!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
